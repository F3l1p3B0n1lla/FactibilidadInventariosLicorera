{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1NPI5PsgffJg"
   },
   "source": [
    "## Proyecto Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import pyodbc\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convertir CSV a Parquet y Guardarlo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ETL archivo 2017PurchasePicesDec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Autenticaci√≥n exitosa. Buckets disponibles: ['licoreradatos']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from google.cloud import storage\n",
    "\n",
    "# Establecer la ruta correcta de las credenciales\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = r\"C:\\\\Proyecto Final Python\\\\arcane-geode-449913-m8-2f7230c7d0b6.json\"\n",
    "\n",
    "# Probar la autenticaci√≥n\n",
    "try:\n",
    "    client = storage.Client()\n",
    "    buckets = list(client.list_buckets())\n",
    "    print(\"‚úÖ Autenticaci√≥n exitosa. Buckets disponibles:\", [bucket.name for bucket in buckets])\n",
    "except Exception as e:\n",
    "    print(\"‚ùå Error de autenticaci√≥n:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo cargado: C:\\\\Proyecto Final Henry\\\\DataBase\\\\2017PurchasePricesDec1.csv\n",
      "\n",
      "üîç Revisi√≥n inicial de los datos:\n",
      "Forma del DataFrame (filas, columnas): (12261, 10)\n",
      "\n",
      "Valores faltantes por columna:\n",
      " Brand             0\n",
      "Description       1\n",
      "Tipo de Licor     1\n",
      "Price             0\n",
      "Size              1\n",
      "Volume            1\n",
      "Classification    0\n",
      "PurchasePrice     0\n",
      "VendorNumber      0\n",
      "VendorName        0\n",
      "dtype: int64\n",
      "\n",
      "Total de filas duplicadas: 0\n",
      "\n",
      "Datos despu√©s de limpieza: (12256, 10)\n",
      "\n",
      "üìå Columnas renombradas correctamente.\n",
      "\n",
      "‚úÖ Archivo guardado en Parquet: C:\\\\Proyecto Final Henry\\\\DataBase\\\\processed\\\\Productos.parquet\n"
     ]
    }
   ],
   "source": [
    "def cargar_csv(ruta_csv):\n",
    "    \"\"\"Carga un archivo CSV en un DataFrame de Pandas.\"\"\"\n",
    "    df = pd.read_csv(ruta_csv)\n",
    "    print(f\"Archivo cargado: {ruta_csv}\")\n",
    "    return df\n",
    "\n",
    "def revisar_datos(df):\n",
    "    \"\"\"Revisa el total de filas y columnas, valores faltantes y duplicados.\"\"\"\n",
    "    print(\"\\nüîç Revisi√≥n inicial de los datos:\")\n",
    "    print(f\"Forma del DataFrame (filas, columnas): {df.shape}\")\n",
    "    print(\"\\nValores faltantes por columna:\\n\", df.isnull().sum())\n",
    "    \n",
    "    duplicados = df.duplicated().sum()\n",
    "    print(f\"\\nTotal de filas duplicadas: {duplicados}\")\n",
    "\n",
    "def limpiar_datos(df):\n",
    "    \"\"\"Limpia los datos eliminando valores nulos y 'unknown' en Size y Volume.\"\"\"\n",
    "    \n",
    "    # Eliminar filas con valores nulos\n",
    "    df = df.dropna()\n",
    "    \n",
    "    # Filtrar filas con \"unknown\" en Size o Volume\n",
    "    unknown_mask = df[\"Size\"].str.contains(\"unknown\", case=False, na=False) | \\\n",
    "                   df[\"Volume\"].astype(str).str.contains(\"unknown\", case=False, na=False)\n",
    "    \n",
    "    df_clean = df[~unknown_mask]\n",
    "    \n",
    "    print(f\"\\nDatos despu√©s de limpieza: {df_clean.shape}\")\n",
    "    return df_clean\n",
    "\n",
    "def renombrar_columnas(df):\n",
    "    \"\"\"Renombra las columnas del DataFrame seg√∫n un formato est√°ndar.\"\"\"\n",
    "    df.columns = [\n",
    "        \"ProductoId\", \"NombreProducto\",\"TipoLicor\", \"Precio\", \"Tamano\", \"Volumen\",\n",
    "        \"Clasificacion\", \"Costo\", \"ProveedorId\", \"NombreProveedor\"\n",
    "    ]\n",
    "    print(\"\\nüìå Columnas renombradas correctamente.\")\n",
    "    return df\n",
    "\n",
    "def guardar_parquet(df, output_path):\n",
    "    \"\"\"Guarda el DataFrame en formato Parquet con compresi√≥n Snappy.\"\"\"\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    df.to_parquet(output_path, engine=\"pyarrow\", compression=\"snappy\")\n",
    "    print(f\"\\n‚úÖ Archivo guardado en Parquet: {output_path}\")\n",
    "\n",
    "\n",
    "# ------------------ üîπ EJECUCI√ìN DEL PROCESO ETL üîπ ------------------\n",
    "ruta_csv = r\"C:\\\\Proyecto Final Henry\\\\DataBase\\\\2017PurchasePricesDec1.csv\"\n",
    "ruta_parquet = r\"C:\\\\Proyecto Final Henry\\\\DataBase\\\\processed\\\\Productos.parquet\"\n",
    "\n",
    "# 1Ô∏è‚É£ Cargar datos\n",
    "df = cargar_csv(ruta_csv)\n",
    "\n",
    "# 2Ô∏è‚É£ Revisar datos\n",
    "revisar_datos(df)\n",
    "\n",
    "# 3Ô∏è‚É£ Limpiar datos\n",
    "df = limpiar_datos(df)\n",
    "\n",
    "# 4Ô∏è‚É£ Renombrar columnas\n",
    "df = renombrar_columnas(df)\n",
    "\n",
    "# 5Ô∏è‚É£ Guardar en Parquet\n",
    "guardar_parquet(df, ruta_parquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ProductoId               NombreProducto           TipoLicor  Precio Tamano  \\\n",
      "0          58  Gekkeikan Black & Gold Sake  Sake/ Shochu/ Soju   12.99  750mL   \n",
      "1          62     Herradura Silver Tequila             Tequila   36.99  750mL   \n",
      "2          63   Herradura Reposado Tequila             Tequila   38.99  750mL   \n",
      "3          72         No. 3 London Dry Gin             Ginebra   34.99  750mL   \n",
      "4          75    Three Olives Tomato Vodka               Vodka   14.99  750mL   \n",
      "\n",
      "  Volumen  Clasificacion  Costo  ProveedorId              NombreProveedor  \n",
      "0     750              1   9.28         8320  SHAW ROSS INT L IMP LTD      \n",
      "1     750              1  28.67         1128  BROWN-FORMAN CORP            \n",
      "2     750              1  30.46         1128  BROWN-FORMAN CORP            \n",
      "3     750              1  26.11         9165  ULTRA BEVERAGE COMPANY LLP   \n",
      "4     750              1  10.94         7245  PROXIMO SPIRITS INC.         \n",
      "Index(['ProductoId', 'NombreProducto', 'TipoLicor', 'Precio', 'Tamano',\n",
      "       'Volumen', 'Clasificacion', 'Costo', 'ProveedorId', 'NombreProveedor'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Cargar un archivo Parquet\n",
    "df = pd.read_parquet(\"C:\\\\Proyecto Final Henry\\\\DataBase\\\\processed\\\\Productos.parquet\")\n",
    "\n",
    "# Mostrar las primeras filas\n",
    "print(df.head())\n",
    "\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subir archivo al bucket a la carpeta Datos Limpios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = r\"C:\\\\Proyecto Final Python\\\\arcane-geode-449913-m8-2f7230c7d0b6.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\\\Proyecto Final Python\\\\arcane-geode-449913-m8-2f7230c7d0b6.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getenv(\"GOOGLE_APPLICATION_CREDENTIALS\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ Archivo C:\\\\Proyecto Final Henry\\\\DataBase\\\\processed\\\\Productos.parquet subido a gs://licoreradatos/Datos Limpios/Productos.parquet\n"
     ]
    }
   ],
   "source": [
    "def subir_a_gcs(bucket_name, source_file_path, folder_name, destination_file_name):\n",
    "    \"\"\"Sube un archivo a una carpeta espec√≠fica dentro de un bucket en Google Cloud Storage.\"\"\"\n",
    "    \n",
    "    # Inicializar el cliente de GCS\n",
    "    storage_client = storage.Client()\n",
    "    \n",
    "    # Obtener el bucket\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    \n",
    "    # Definir la ruta completa en el bucket (carpeta + nombre de archivo)\n",
    "    destination_blob_name = f\"{folder_name}/{destination_file_name}\".strip(\"/\")\n",
    "    \n",
    "    # Crear un blob (objeto en el bucket)\n",
    "    blob = bucket.blob(destination_blob_name)\n",
    "    \n",
    "    # Subir el archivo\n",
    "    blob.upload_from_filename(source_file_path)\n",
    "\n",
    "    print(f\"\\nüöÄ Archivo {source_file_path} subido a gs://{bucket_name}/{destination_blob_name}\")\n",
    "\n",
    "# Datos espec√≠ficos de tu bucket y archivo\n",
    "bucket_name = \"licoreradatos\"  # Tu bucket en GCS\n",
    "folder_name = \"Datos Limpios\"  # Carpeta dentro del bucket\n",
    "source_file_path = r\"C:\\\\Proyecto Final Henry\\\\DataBase\\\\processed\\\\Productos.parquet\"  # Archivo local\n",
    "destination_file_name = \"Productos.parquet\"  # Nombre final en el bucket\n",
    "\n",
    "# Subir archivo a GCS\n",
    "subir_a_gcs(bucket_name, source_file_path, folder_name, destination_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ETL archivo BegInvFINAL12312026"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo cargado: C:\\\\Proyecto Final Henry\\\\DataBase\\\\BegInvFINAL123120161.csv\n",
      "\n",
      "üîç Revisi√≥n inicial de los datos:\n",
      "Forma del DataFrame (filas, columnas): (206529, 10)\n",
      "\n",
      "Valores faltantes por columna:\n",
      " InventoryId      0\n",
      "Store            0\n",
      "City             0\n",
      "Brand            0\n",
      "Description      0\n",
      "Size             0\n",
      "onHand           0\n",
      "Price            0\n",
      "startDate        0\n",
      "PurchasePrice    0\n",
      "dtype: int64\n",
      "\n",
      "Total de filas duplicadas: 0\n",
      "\n",
      "üìå Columnas renombradas correctamente.\n",
      "\n",
      "‚úÖ Archivo guardado en Parquet: C:\\\\Proyecto Final Henry\\\\DataBase\\\\processed\\\\InventarioInicial.parquet\n"
     ]
    }
   ],
   "source": [
    "def cargar_csv(ruta_csv1):\n",
    "    \"\"\"Carga un archivo CSV en un DataFrame de Pandas.\"\"\"\n",
    "    df1 = pd.read_csv(ruta_csv1)\n",
    "    print(f\"Archivo cargado: {ruta_csv1}\")\n",
    "    return df1\n",
    "\n",
    "def revisar_datos(df1):\n",
    "    \"\"\"Revisa el total de filas y columnas, valores faltantes y duplicados.\"\"\"\n",
    "    print(\"\\nüîç Revisi√≥n inicial de los datos:\")\n",
    "    print(f\"Forma del DataFrame (filas, columnas): {df1.shape}\")\n",
    "    print(\"\\nValores faltantes por columna:\\n\", df1.isnull().sum())\n",
    "    \n",
    "    duplicados = df1.duplicated().sum()\n",
    "    print(f\"\\nTotal de filas duplicadas: {duplicados}\")\n",
    "\n",
    "def renombrar_columnas(df1):\n",
    "    \"\"\"Renombra las columnas del DataFrame seg√∫n un formato est√°ndar.\"\"\"\n",
    "    df1.columns = [\n",
    "        \"InventarioInicialId\", \"Tienda\", \"Ciudad\", \"ProductoId\", \"NombreProducto\",\n",
    "        \"Tamano\", \"Cantidad\", \"Precio\", \"FechaInicio\", \"Costo\"\n",
    "    ]\n",
    "    print(\"\\nüìå Columnas renombradas correctamente.\")\n",
    "    return df1  # ¬°Aseg√∫rate de devolver el DataFrame!\n",
    "\n",
    "def guardar_parquet(df1, output_path):\n",
    "    \"\"\"Guarda el DataFrame en formato Parquet con compresi√≥n Snappy.\"\"\"\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    df1.to_parquet(output_path, engine=\"pyarrow\", compression=\"snappy\")\n",
    "    print(f\"\\n‚úÖ Archivo guardado en Parquet: {output_path}\")\n",
    "\n",
    "# ------------------ üîπ EJECUCI√ìN DEL PROCESO ETL üîπ ------------------\n",
    "ruta_csv1 = r\"C:\\\\Proyecto Final Henry\\\\DataBase\\\\BegInvFINAL123120161.csv\"\n",
    "ruta_parquet1 = r\"C:\\\\Proyecto Final Henry\\\\DataBase\\\\processed\\\\InventarioInicial.parquet\"\n",
    "\n",
    "# 1Ô∏è‚É£ Cargar datos\n",
    "df1 = cargar_csv(ruta_csv1)\n",
    "\n",
    "# 2Ô∏è‚É£ Revisar datos\n",
    "revisar_datos(df1)\n",
    "\n",
    "# 3Ô∏è‚É£ Renombrar columnas\n",
    "df1 = renombrar_columnas(df1)\n",
    "\n",
    "# 4Ô∏è‚É£ Guardar en Parquet\n",
    "guardar_parquet(df1, ruta_parquet1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  InventarioInicialId  Tienda        Ciudad  ProductoId  \\\n",
      "0   1_HARDERSFIELD_58       1  HARDERSFIELD          58   \n",
      "1   1_HARDERSFIELD_60       1  HARDERSFIELD          60   \n",
      "2   1_HARDERSFIELD_62       1  HARDERSFIELD          62   \n",
      "3   1_HARDERSFIELD_63       1  HARDERSFIELD          63   \n",
      "4   1_HARDERSFIELD_72       1  HARDERSFIELD          72   \n",
      "\n",
      "                NombreProducto Tamano  Cantidad  Precio FechaInicio  Costo  \n",
      "0  Gekkeikan Black & Gold Sake  750mL         8   12.99    1/1/2016   9.28  \n",
      "1       Canadian Club 1858 VAP  750mL         7   10.99    1/1/2016   7.40  \n",
      "2     Herradura Silver Tequila  750mL         6   36.99    1/1/2016  28.67  \n",
      "3   Herradura Reposado Tequila  750mL         3   38.99    1/1/2016  30.46  \n",
      "4         No. 3 London Dry Gin  750mL         6   34.99    1/1/2016  26.11  \n",
      "Index(['InventarioInicialId', 'Tienda', 'Ciudad', 'ProductoId',\n",
      "       'NombreProducto', 'Tamano', 'Cantidad', 'Precio', 'FechaInicio',\n",
      "       'Costo'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Cargar un archivo Parquet\n",
    "df = pd.read_parquet(\"C:\\\\Proyecto Final Henry\\\\DataBase\\\\processed\\\\InventarioInicial.parquet\")\n",
    "\n",
    "# Mostrar las primeras filas\n",
    "print(df.head())\n",
    "\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subir archivo al bucket a la carpeta Datos Limpios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ Archivo C:\\\\Proyecto Final Henry\\\\DataBase\\\\processed\\\\InventarioInicial.parquet subido a gs://licoreradatos/Datos Limpios/InventarioInicial.parquet\n"
     ]
    }
   ],
   "source": [
    "def subir_a_gcs(bucket_name, source_file_path1, folder_name, destination_file_name1):\n",
    "    \"\"\"Sube un archivo a una carpeta espec√≠fica dentro de un bucket en Google Cloud Storage.\"\"\"\n",
    "    \n",
    "    # Inicializar el cliente de GCS\n",
    "    storage_client = storage.Client()\n",
    "    \n",
    "    # Obtener el bucket\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    \n",
    "    # Definir la ruta completa en el bucket (carpeta + nombre de archivo)\n",
    "    destination_blob_name = f\"{folder_name}/{destination_file_name1}\".strip(\"/\")\n",
    "    \n",
    "    # Crear un blob (objeto en el bucket)\n",
    "    blob = bucket.blob(destination_blob_name)\n",
    "    \n",
    "    # Subir el archivo\n",
    "    blob.upload_from_filename(source_file_path1)\n",
    "\n",
    "    print(f\"\\nüöÄ Archivo {source_file_path1} subido a gs://{bucket_name}/{destination_blob_name}\")\n",
    "\n",
    "# Datos espec√≠ficos de tu bucket y archivo\n",
    "bucket_name = \"licoreradatos\"  # Tu bucket en GCS\n",
    "folder_name = \"Datos Limpios\"  # Carpeta dentro del bucket\n",
    "source_file_path1 = r\"C:\\\\Proyecto Final Henry\\\\DataBase\\\\processed\\\\InventarioInicial.parquet\"  # Archivo local\n",
    "destination_file_name1 = \"InventarioInicial.parquet\"  # Nombre final en el bucket\n",
    "\n",
    "# Subir archivo a GCS\n",
    "subir_a_gcs(bucket_name, source_file_path1, folder_name, destination_file_name1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ETL archivo EndInvFINAL12312026"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Archivo cargado: C:\\\\Proyecto Final Henry\\\\DataBase\\\\EndInvFINAL123120161.csv\n",
      "\n",
      "üîç Revisi√≥n inicial de los datos:\n",
      "Forma del DataFrame (filas, columnas): (224489, 10)\n",
      "\n",
      "Valores faltantes por columna:\n",
      " InventoryId         0\n",
      "Store               0\n",
      "City             1284\n",
      "Brand               0\n",
      "Description         0\n",
      "Size                0\n",
      "onHand              0\n",
      "Price               0\n",
      "endDate             0\n",
      "PurchasePrice       0\n",
      "dtype: int64\n",
      "\n",
      "Total de filas duplicadas: 0\n",
      "\n",
      "üìå Columnas renombradas correctamente.\n",
      "\n",
      "‚úÖ Archivo guardado en Parquet: C:\\\\Proyecto Final Henry\\\\DataBase\\\\processed\\\\InventarioFinal.parquet\n"
     ]
    }
   ],
   "source": [
    "def cargar_csv(ruta_csv3):\n",
    "    \"\"\"Carga un archivo CSV en un DataFrame de Pandas.\"\"\"\n",
    "    df3 = pd.read_csv(ruta_csv3)\n",
    "    print(f\"‚úÖ Archivo cargado: {ruta_csv3}\")\n",
    "    return df3\n",
    "\n",
    "def revisar_datos(df3):\n",
    "    \"\"\"Revisa el total de filas y columnas, valores faltantes y duplicados.\"\"\"\n",
    "    print(\"\\nüîç Revisi√≥n inicial de los datos:\")\n",
    "    print(f\"Forma del DataFrame (filas, columnas): {df3.shape}\")\n",
    "    print(\"\\nValores faltantes por columna:\\n\", df3.isnull().sum())\n",
    "    \n",
    "    duplicados = df3.duplicated().sum()\n",
    "    print(f\"\\nTotal de filas duplicadas: {duplicados}\")\n",
    "\n",
    "def replace_nan_city(df3, store_number, city_name):\n",
    "    \"\"\"Reemplaza los valores NaN en 'City' con el nombre de la ciudad si Store es el especificado.\"\"\"\n",
    "    df3.loc[(df3['Store'] == store_number) & (df3['City'].isna()), 'City'] = city_name\n",
    "    return df3\n",
    "\n",
    "def limpiar_datos(df3):\n",
    "    \"\"\"Limpia los datos eliminando valores nulos y reemplazando valores faltantes en City.\"\"\"\n",
    "    df3 = replace_nan_city(df3, 46, \"TYWARDREATH\")  # ‚úÖ Corrige valores de City\n",
    "    return df3\n",
    "\n",
    "def renombrar_columnas(df3):\n",
    "    \"\"\"Renombra las columnas en el orden especificado.\"\"\"\n",
    "    columnas_originales = df3.columns.tolist()\n",
    "    \n",
    "    nombres_nuevos = [\n",
    "        \"InventarioFinalId\", \"Tienda\", \"Ciudad\", \"ProductoId\", \"NombreProducto\",\n",
    "        \"Tamano\", \"Cantidad\", \"Precio\", \"FechaFin\", \"Costo\"\n",
    "    ]\n",
    "    \n",
    "    # Si el n√∫mero de columnas no coincide, se evita el error\n",
    "    if len(columnas_originales) != len(nombres_nuevos):\n",
    "        print(\"‚ö†Ô∏è Advertencia: La cantidad de columnas no coincide con los nombres nuevos.\")\n",
    "        print(f\"Columnas originales: {columnas_originales}\")\n",
    "        return df3  # Retorna sin cambios\n",
    "\n",
    "    df3.columns = nombres_nuevos  # ‚úÖ Se renombraron las columnas correctamente\n",
    "    print(\"\\nüìå Columnas renombradas correctamente.\")\n",
    "    return df3\n",
    "\n",
    "def guardar_parquet(df3, output_path):\n",
    "    \"\"\"Guarda el DataFrame en formato Parquet con compresi√≥n Snappy.\"\"\"\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    df3.to_parquet(output_path, engine=\"pyarrow\", compression=\"snappy\")\n",
    "    print(f\"\\n‚úÖ Archivo guardado en Parquet: {output_path}\")\n",
    "\n",
    "# ------------------ üîπ EJECUCI√ìN DEL PROCESO ETL üîπ ------------------\n",
    "ruta_csv3 = r\"C:\\\\Proyecto Final Henry\\\\DataBase\\\\EndInvFINAL123120161.csv\"\n",
    "ruta_parquet3 = r\"C:\\\\Proyecto Final Henry\\\\DataBase\\\\processed\\\\InventarioFinal.parquet\"\n",
    "\n",
    "# 1Ô∏è‚É£ Cargar datos\n",
    "df3 = cargar_csv(ruta_csv3)\n",
    "\n",
    "# 2Ô∏è‚É£ Revisar datos\n",
    "revisar_datos(df3)\n",
    "\n",
    "# 3Ô∏è‚É£ Limpiar datos\n",
    "df3 = limpiar_datos(df3)  # ‚úÖ Se asegura de que df3 no sea None\n",
    "\n",
    "# 4Ô∏è‚É£ Renombrar columnas antes de guardar\n",
    "df3 = renombrar_columnas(df3)\n",
    "\n",
    "# 5Ô∏è‚É£ Guardar en Parquet\n",
    "guardar_parquet(df3, ruta_parquet3)  # ‚úÖ Ahora df3 es v√°lido y no dar√° error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   InventarioFinalId  Tienda        Ciudad  ProductoId  \\\n",
      "0  1_HARDERSFIELD_58       1  HARDERSFIELD          58   \n",
      "1  1_HARDERSFIELD_62       1  HARDERSFIELD          62   \n",
      "2  1_HARDERSFIELD_63       1  HARDERSFIELD          63   \n",
      "3  1_HARDERSFIELD_72       1  HARDERSFIELD          72   \n",
      "4  1_HARDERSFIELD_75       1  HARDERSFIELD          75   \n",
      "\n",
      "                NombreProducto Tamano  Cantidad  Precio    FechaFin  Costo  \n",
      "0  Gekkeikan Black & Gold Sake  750mL        11   12.99  12/31/2016   9.28  \n",
      "1     Herradura Silver Tequila  750mL         7   36.99  12/31/2016  28.67  \n",
      "2   Herradura Reposado Tequila  750mL         7   38.99  12/31/2016  30.46  \n",
      "3         No. 3 London Dry Gin  750mL         4   34.99  12/31/2016  26.11  \n",
      "4    Three Olives Tomato Vodka  750mL         7   14.99  12/31/2016  10.94  \n",
      "Index(['InventarioFinalId', 'Tienda', 'Ciudad', 'ProductoId', 'NombreProducto',\n",
      "       'Tamano', 'Cantidad', 'Precio', 'FechaFin', 'Costo'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Cargar un archivo Parquet\n",
    "df = pd.read_parquet(\"C:\\\\Proyecto Final Henry\\\\DataBase\\\\processed\\\\InventarioFinal.parquet\")\n",
    "\n",
    "# Mostrar las primeras filas\n",
    "print(df.head())\n",
    "\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subir archivo al bucket a la carpeta Datos Limpios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ Archivo C:\\\\Proyecto Final Henry\\\\DataBase\\\\processed\\\\InventarioFinal.parquet subido a gs://licoreradatos/Datos Limpios/InventarioFinal.parquet\n"
     ]
    }
   ],
   "source": [
    "def subir_a_gcs(bucket_name, source_file_path3, folder_name, destination_file_name3):\n",
    "    \"\"\"Sube un archivo a una carpeta espec√≠fica dentro de un bucket en Google Cloud Storage.\"\"\"\n",
    "    \n",
    "    # Inicializar el cliente de GCS\n",
    "    storage_client = storage.Client()\n",
    "    \n",
    "    # Obtener el bucket\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    \n",
    "    # Definir la ruta completa en el bucket (carpeta + nombre de archivo)\n",
    "    destination_blob_name = f\"{folder_name}/{destination_file_name3}\".strip(\"/\")\n",
    "    \n",
    "    # Crear un blob (objeto en el bucket)\n",
    "    blob = bucket.blob(destination_blob_name)\n",
    "    \n",
    "    # Subir el archivo\n",
    "    blob.upload_from_filename(source_file_path3)\n",
    "\n",
    "    print(f\"\\nüöÄ Archivo {source_file_path3} subido a gs://{bucket_name}/{destination_blob_name}\")\n",
    "\n",
    "# Datos espec√≠ficos de tu bucket y archivo\n",
    "bucket_name = \"licoreradatos\"  # Tu bucket en GCS\n",
    "folder_name = \"Datos Limpios\"  # Carpeta dentro del bucket\n",
    "source_file_path3 = r\"C:\\\\Proyecto Final Henry\\\\DataBase\\\\processed\\\\InventarioFinal.parquet\"  # Archivo local\n",
    "destination_file_name3 = \"InventarioFinal.parquet\"  # Nombre final en el bucket\n",
    "\n",
    "# Subir archivo a GCS\n",
    "subir_a_gcs(bucket_name, source_file_path3, folder_name, destination_file_name3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ETL archivo InvoicePurchases12312016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Archivo cargado: C:\\\\Proyecto Final Henry\\\\DataBase\\\\InvoicePurchases12312016.csv\n",
      "\n",
      "üîç Revisi√≥n inicial de los datos:\n",
      "Forma del DataFrame (filas, columnas): (5543, 10)\n",
      "\n",
      "Valores faltantes por columna:\n",
      " VendorNumber       0\n",
      "VendorName         0\n",
      "InvoiceDate        0\n",
      "PONumber           0\n",
      "PODate             0\n",
      "PayDate            0\n",
      "Quantity           0\n",
      "Dollars            0\n",
      "Freight            0\n",
      "Approval        5169\n",
      "dtype: int64\n",
      "\n",
      "Total de filas duplicadas: 0\n",
      "\n",
      "üìå Valores √∫nicos en la columna 'Approval':\n",
      "[nan 'Frank Delahunt']\n",
      "\n",
      "üöÆ Columna 'Approval' eliminada.\n",
      "\n",
      "‚úÖ Datos limpiados correctamente.\n",
      "\n",
      "üìå Columnas renombradas correctamente.\n",
      "\n",
      "‚úÖ Archivo guardado en Parquet: C:\\\\Proyecto Final Henry\\\\DataBase\\\\processed\\\\Compras.parquet\n"
     ]
    }
   ],
   "source": [
    "def cargar_csv(ruta_csv4):\n",
    "    \"\"\"Carga un archivo CSV en un DataFrame de Pandas.\"\"\"\n",
    "    df4 = pd.read_csv(ruta_csv4)\n",
    "    print(f\"‚úÖ Archivo cargado: {ruta_csv4}\")\n",
    "    return df4\n",
    "\n",
    "def revisar_datos(df4):\n",
    "    \"\"\"Revisa el total de filas y columnas, valores faltantes y duplicados.\"\"\"\n",
    "    print(\"\\nüîç Revisi√≥n inicial de los datos:\")\n",
    "    print(f\"Forma del DataFrame (filas, columnas): {df4.shape}\")\n",
    "    print(\"\\nValores faltantes por columna:\\n\", df4.isnull().sum())\n",
    "    \n",
    "    duplicados = df4.duplicated().sum()\n",
    "    print(f\"\\nTotal de filas duplicadas: {duplicados}\")\n",
    "\n",
    "    # Mostrar valores √∫nicos en la columna 'Approval' si existe\n",
    "    if \"Approval\" in df4.columns:\n",
    "        unique_values = df4[\"Approval\"].unique()\n",
    "        print(\"\\nüìå Valores √∫nicos en la columna 'Approval':\")\n",
    "        print(unique_values)\n",
    "\n",
    "def limpiar_datos(df4):\n",
    "    \"\"\"Limpia los datos eliminando valores nulos y la columna Approval.\"\"\"\n",
    "    # Eliminar la columna 'Approval' si existe\n",
    "    if \"Approval\" in df4.columns:\n",
    "        df4 = df4.drop(columns=[\"Approval\"])\n",
    "        print(\"\\nüöÆ Columna 'Approval' eliminada.\")\n",
    "\n",
    "    # Eliminar filas con valores nulos\n",
    "    df4 = df4.dropna()\n",
    "    \n",
    "    print(\"\\n‚úÖ Datos limpiados correctamente.\")\n",
    "    return df4  # Asegurar que devuelve el DataFrame limpio\n",
    "\n",
    "def renombrar_columnas(df4):\n",
    "    \"\"\"Renombra las columnas del DataFrame seg√∫n un formato est√°ndar.\"\"\"\n",
    "    df4.columns = [\n",
    "        \"ProveedorId\", \"NombreProveedor\", \"FechaFactura\", \"CompraId\", \n",
    "        \"FechaCompra\", \"FechaPago\", \"Cantidad\", \"CostoTotal\", \"CostoEnvio\"\n",
    "    ]\n",
    "    print(\"\\nüìå Columnas renombradas correctamente.\")\n",
    "    return df4  # Asegurar que devuelve el DataFrame actualizado\n",
    "\n",
    "def guardar_parquet(df4, output_path):\n",
    "    \"\"\"Guarda el DataFrame en formato Parquet con compresi√≥n Snappy.\"\"\"\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    df4.to_parquet(output_path, engine=\"pyarrow\", compression=\"snappy\")\n",
    "    print(f\"\\n‚úÖ Archivo guardado en Parquet: {output_path}\")\n",
    "\n",
    "# ------------------ üîπ EJECUCI√ìN DEL PROCESO ETL üîπ ------------------\n",
    "ruta_csv4 = r\"C:\\\\Proyecto Final Henry\\\\DataBase\\\\InvoicePurchases12312016.csv\"\n",
    "ruta_parquet4 = r\"C:\\\\Proyecto Final Henry\\\\DataBase\\\\processed\\\\Compras.parquet\"\n",
    "\n",
    "# 1Ô∏è‚É£ Cargar datos\n",
    "df4 = cargar_csv(ruta_csv4)\n",
    "\n",
    "# 2Ô∏è‚É£ Revisar datos\n",
    "revisar_datos(df4)\n",
    "\n",
    "# 3Ô∏è‚É£ Limpiar datos\n",
    "df4 = limpiar_datos(df4)\n",
    "\n",
    "# 4Ô∏è‚É£ Renombrar columnas\n",
    "df4 = renombrar_columnas(df4)\n",
    "\n",
    "# 5Ô∏è‚É£ Guardar en Parquet\n",
    "guardar_parquet(df4, ruta_parquet4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ProveedorId              NombreProveedor FechaFactura  CompraId  \\\n",
      "0          105  ALTAMAR BRANDS LLC            2016-01-04      8124   \n",
      "1         4466  AMERICAN VINTAGE BEVERAGE     2016-01-07      8137   \n",
      "2          388  ATLANTIC IMPORTING COMPANY    2016-01-09      8169   \n",
      "3          480  BACARDI USA INC               2016-01-12      8106   \n",
      "4          516  BANFI PRODUCTS CORP           2016-01-07      8170   \n",
      "\n",
      "  FechaCompra   FechaPago  Cantidad  CostoTotal  CostoEnvio  \n",
      "0  2015-12-21  2016-02-16         6      214.26        3.47  \n",
      "1  2015-12-22  2016-02-21        15      140.55        8.57  \n",
      "2  2015-12-24  2016-02-16         5      106.60        4.61  \n",
      "3  2015-12-20  2016-02-05     10100   137483.78     2935.20  \n",
      "4  2015-12-24  2016-02-12      1935    15527.25      429.20  \n",
      "Index(['ProveedorId', 'NombreProveedor', 'FechaFactura', 'CompraId',\n",
      "       'FechaCompra', 'FechaPago', 'Cantidad', 'CostoTotal', 'CostoEnvio'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Cargar un archivo Parquet\n",
    "df = pd.read_parquet(\"C:\\\\Proyecto Final Henry\\\\DataBase\\\\processed\\\\Compras.parquet\")\n",
    "\n",
    "# Mostrar las primeras filas\n",
    "print(df.head())\n",
    "\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subir archivo al bucket a la carpeta Datos Limpios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ Archivo C:\\\\Proyecto Final Henry\\\\DataBase\\\\processed\\\\Compras.parquet subido a gs://licoreradatos/Datos Limpios/Compras.parquet\n"
     ]
    }
   ],
   "source": [
    "def subir_a_gcs(bucket_name, source_file_path4, folder_name, destination_file_name4):\n",
    "    \"\"\"Sube un archivo a una carpeta espec√≠fica dentro de un bucket en Google Cloud Storage.\"\"\"\n",
    "    \n",
    "    # Inicializar el cliente de GCS\n",
    "    storage_client = storage.Client()\n",
    "    \n",
    "    # Obtener el bucket\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    \n",
    "    # Definir la ruta completa en el bucket (carpeta + nombre de archivo)\n",
    "    destination_blob_name = f\"{folder_name}/{destination_file_name4}\".strip(\"/\")\n",
    "    \n",
    "    # Crear un blob (objeto en el bucket)\n",
    "    blob = bucket.blob(destination_blob_name)\n",
    "    \n",
    "    # Subir el archivo\n",
    "    blob.upload_from_filename(source_file_path4)\n",
    "\n",
    "    print(f\"\\nüöÄ Archivo {source_file_path4} subido a gs://{bucket_name}/{destination_blob_name}\")\n",
    "\n",
    "# Datos espec√≠ficos de tu bucket y archivo\n",
    "bucket_name = \"licoreradatos\"  # Tu bucket en GCS\n",
    "folder_name = \"Datos Limpios\"  # Carpeta dentro del bucket\n",
    "source_file_path4 = r\"C:\\\\Proyecto Final Henry\\\\DataBase\\\\processed\\\\Compras.parquet\"  # Archivo local\n",
    "destination_file_name4 = \"Compras.parquet\"  # Nombre final en el bucket\n",
    "\n",
    "# Subir archivo a GCS\n",
    "subir_a_gcs(bucket_name, source_file_path4, folder_name, destination_file_name4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ETL archivo PurchasesFINAL12312016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Archivo cargado: C:\\\\Proyecto Final Henry\\\\DataBase\\\\PurchasesFINAL12312016.csv\n",
      "\n",
      "üîç Revisi√≥n inicial de los datos:\n",
      "Forma del DataFrame (filas, columnas): (2372474, 16)\n",
      "\n",
      "Valores faltantes por columna:\n",
      " InventoryId       0\n",
      "Store             0\n",
      "Brand             0\n",
      "Description       0\n",
      "Size              3\n",
      "VendorNumber      0\n",
      "VendorName        0\n",
      "PONumber          0\n",
      "PODate            0\n",
      "ReceivingDate     0\n",
      "InvoiceDate       0\n",
      "PayDate           0\n",
      "PurchasePrice     0\n",
      "Quantity          0\n",
      "Dollars           0\n",
      "Classification    0\n",
      "dtype: int64\n",
      "\n",
      "Total de filas duplicadas: 0\n",
      "\n",
      "üöÆ 3 filas con valores NaN en 'Size' eliminadas.\n",
      "\n",
      "‚úÖ Datos limpiados correctamente.\n",
      "\n",
      "üìå Columnas renombradas correctamente.\n",
      "\n",
      "üîé Filas finales esperadas: 2,372,471\n",
      "üîé Filas actuales despu√©s de limpieza: 2372471\n",
      "\n",
      "‚úÖ Archivo guardado en Parquet: C:\\\\Proyecto Final Henry\\\\DataBase\\\\processed\\\\DetalleCompra.parquet\n"
     ]
    }
   ],
   "source": [
    "def cargar_csv(ruta_csv5):\n",
    "    \"\"\"Carga un archivo CSV en un DataFrame de Pandas.\"\"\"\n",
    "    df5 = pd.read_csv(ruta_csv5)\n",
    "    print(f\"‚úÖ Archivo cargado: {ruta_csv5}\")\n",
    "    return df5\n",
    "\n",
    "def revisar_datos(df5):\n",
    "    \"\"\"Revisa el total de filas y columnas, valores faltantes y duplicados.\"\"\"\n",
    "    print(\"\\nüîç Revisi√≥n inicial de los datos:\")\n",
    "    print(f\"Forma del DataFrame (filas, columnas): {df5.shape}\")\n",
    "    print(\"\\nValores faltantes por columna:\\n\", df5.isnull().sum())\n",
    "    \n",
    "    duplicados = df5.duplicated().sum()\n",
    "    print(f\"\\nTotal de filas duplicadas: {duplicados}\")\n",
    "\n",
    "def limpiar_datos(df5):\n",
    "    \"\"\"Elimina valores nulos en la columna 'Tamano' y filas duplicadas sin afectar el conteo final esperado.\"\"\"\n",
    "    # Verificar si la columna \"Tamano\" existe y corregir posibles errores en el nombre\n",
    "    col_tamano = \"Tamano\" if \"Tamano\" in df5.columns else \"Size\" if \"Size\" in df5.columns else None\n",
    "\n",
    "    if col_tamano:\n",
    "        filas_size_nulas = df5[col_tamano].isnull().sum()\n",
    "        df5 = df5.dropna(subset=[col_tamano])\n",
    "        print(f\"\\nüöÆ {filas_size_nulas} filas con valores NaN en '{col_tamano}' eliminadas.\")\n",
    "\n",
    "    print(\"\\n‚úÖ Datos limpiados correctamente.\")\n",
    "    return df5  \n",
    "\n",
    "def renombrar_columnas(df):\n",
    "    \"\"\"Renombra las columnas del DataFrame seg√∫n un formato est√°ndar.\"\"\"\n",
    "    df5.columns = [\n",
    "        \"InventarioIncialId\", \"Tienda\", \"ProductoId\", \"NombreProducto\", \"Tamano\",\n",
    "        \"ProveedorId\", \"NombreProveedor\", \"CompraId\", \"FechaCompra\", \"FechaIngreso\",\n",
    "        \"FechaFactura\", \"FechaPago\", \"CostoUnitario\", \"Cantidad\", \"CostoTotal\", \"Clasificacion\"\n",
    "    ]\n",
    "    print(\"\\nüìå Columnas renombradas correctamente.\")\n",
    "    return df5  \n",
    "\n",
    "def guardar_parquet(df5, output_path):\n",
    "    \"\"\"Guarda el DataFrame en formato Parquet con compresi√≥n Snappy.\"\"\"\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    df5.to_parquet(output_path, engine=\"pyarrow\", compression=\"snappy\")\n",
    "    print(f\"\\n‚úÖ Archivo guardado en Parquet: {output_path}\")\n",
    "\n",
    "# ------------------ üîπ EJECUCI√ìN DEL PROCESO ETL üîπ ------------------\n",
    "ruta_csv5 = r\"C:\\\\Proyecto Final Henry\\\\DataBase\\\\PurchasesFINAL12312016.csv\"\n",
    "ruta_parquet5 = r\"C:\\\\Proyecto Final Henry\\\\DataBase\\\\processed\\\\DetalleCompra.parquet\"\n",
    "\n",
    "# 1Ô∏è‚É£ Cargar datos\n",
    "df5 = cargar_csv(ruta_csv5)\n",
    "\n",
    "# 2Ô∏è‚É£ Revisar datos\n",
    "revisar_datos(df5)\n",
    "\n",
    "# 3Ô∏è‚É£ Limpiar datos (solo eliminamos valores nulos en 'Tamano')\n",
    "df5 = limpiar_datos(df5)\n",
    "\n",
    "# 4Ô∏è‚É£ Renombrar columnas\n",
    "df5 = renombrar_columnas(df5)\n",
    "\n",
    "# 5Ô∏è‚É£ Verificar el n√∫mero de filas despu√©s de la limpieza\n",
    "print(f\"\\nüîé Filas finales esperadas: 2,372,471\")\n",
    "print(f\"üîé Filas actuales despu√©s de limpieza: {df5.shape[0]}\")\n",
    "\n",
    "# 6Ô∏è‚É£ Guardar en Parquet\n",
    "guardar_parquet(df5, ruta_parquet5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    InventarioIncialId  Tienda  ProductoId                NombreProducto  \\\n",
      "0    69_MOUNTMEND_8412      69        8412     Tequila Ocho Plata Fresno   \n",
      "1     30_CULCHETH_5255      30        5255  TGI Fridays Ultimte Mudslide   \n",
      "2    34_PITMERDEN_5215      34        5215  TGI Fridays Long Island Iced   \n",
      "3  1_HARDERSFIELD_5255       1        5255  TGI Fridays Ultimte Mudslide   \n",
      "4    76_DONCASTER_2034      76        2034     Glendalough Double Barrel   \n",
      "\n",
      "  Tamano  ProveedorId              NombreProveedor  CompraId FechaCompra  \\\n",
      "0  750mL          105  ALTAMAR BRANDS LLC               8124  2015-12-21   \n",
      "1  1.75L         4466  AMERICAN VINTAGE BEVERAGE        8137  2015-12-22   \n",
      "2  1.75L         4466  AMERICAN VINTAGE BEVERAGE        8137  2015-12-22   \n",
      "3  1.75L         4466  AMERICAN VINTAGE BEVERAGE        8137  2015-12-22   \n",
      "4  750mL          388  ATLANTIC IMPORTING COMPANY       8169  2015-12-24   \n",
      "\n",
      "  FechaIngreso FechaFactura   FechaPago  CostoUnitario  Cantidad  CostoTotal  \\\n",
      "0   2016-01-02   2016-01-04  2016-02-16          35.71         6      214.26   \n",
      "1   2016-01-01   2016-01-07  2016-02-21           9.35         4       37.40   \n",
      "2   2016-01-02   2016-01-07  2016-02-21           9.41         5       47.05   \n",
      "3   2016-01-01   2016-01-07  2016-02-21           9.35         6       56.10   \n",
      "4   2016-01-02   2016-01-09  2016-02-16          21.32         5      106.60   \n",
      "\n",
      "   Clasificacion  \n",
      "0              1  \n",
      "1              1  \n",
      "2              1  \n",
      "3              1  \n",
      "4              1  \n",
      "Index(['InventarioIncialId', 'Tienda', 'ProductoId', 'NombreProducto',\n",
      "       'Tamano', 'ProveedorId', 'NombreProveedor', 'CompraId', 'FechaCompra',\n",
      "       'FechaIngreso', 'FechaFactura', 'FechaPago', 'CostoUnitario',\n",
      "       'Cantidad', 'CostoTotal', 'Clasificacion'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Cargar un archivo Parquet\n",
    "df = pd.read_parquet(\"C:\\\\Proyecto Final Henry\\\\DataBase\\\\processed\\\\DetalleCompra.parquet\")\n",
    "\n",
    "# Mostrar las primeras filas\n",
    "print(df.head())\n",
    "\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subir archivo al bucket a la carpeta Datos Limpios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ Archivo C:\\\\Proyecto Final Henry\\\\DataBase\\\\processed\\\\DetalleCompra.parquet subido a gs://licoreradatos/Datos Limpios/DetalleCompra.parquet\n"
     ]
    }
   ],
   "source": [
    "def subir_a_gcs(bucket_name, source_file_path5, folder_name, destination_file_name5):\n",
    "    \"\"\"Sube un archivo a una carpeta espec√≠fica dentro de un bucket en Google Cloud Storage.\"\"\"\n",
    "    \n",
    "    # Inicializar el cliente de GCS\n",
    "    storage_client = storage.Client()\n",
    "    \n",
    "    # Obtener el bucket\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    \n",
    "    # Definir la ruta completa en el bucket (carpeta + nombre de archivo)\n",
    "    destination_blob_name = f\"{folder_name}/{destination_file_name5}\".strip(\"/\")\n",
    "    \n",
    "    # Crear un blob (objeto en el bucket)\n",
    "    blob = bucket.blob(destination_blob_name)\n",
    "    \n",
    "    # Subir el archivo\n",
    "    blob.upload_from_filename(source_file_path5)\n",
    "\n",
    "    print(f\"\\nüöÄ Archivo {source_file_path5} subido a gs://{bucket_name}/{destination_blob_name}\")\n",
    "\n",
    "# Datos espec√≠ficos de tu bucket y archivo\n",
    "bucket_name = \"licoreradatos\"  # Tu bucket en GCS\n",
    "folder_name = \"Datos Limpios\"  # Carpeta dentro del bucket\n",
    "source_file_path5 = r\"C:\\\\Proyecto Final Henry\\\\DataBase\\\\processed\\\\DetalleCompra.parquet\"  # Archivo local\n",
    "destination_file_name5 = \"DetalleCompra.parquet\"  # Nombre final en el bucket\n",
    "\n",
    "# Subir archivo a GCS\n",
    "subir_a_gcs(bucket_name, source_file_path5, folder_name, destination_file_name5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ETL archivo SalesFINAL12312016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Archivo cargado: C:\\\\Proyecto Final Henry\\\\DataBase\\\\SalesFINAL123120161.csv\n",
      "\n",
      "üîç Revisi√≥n inicial de los datos:\n",
      "Forma del DataFrame (filas, columnas): (1048575, 15)\n",
      "\n",
      "Valores faltantes por columna:\n",
      " InventoryId       0\n",
      "Store             0\n",
      "Brand             0\n",
      "Description       0\n",
      "Size              0\n",
      "SalesQuantity     0\n",
      "SalesDollars      0\n",
      "SalesPrice        0\n",
      "SalesDate         0\n",
      "Volume            0\n",
      "Classification    0\n",
      "ExciseTax         0\n",
      "VendorNo          0\n",
      "VendorName        0\n",
      "PurchasePrice     0\n",
      "dtype: int64\n",
      "\n",
      "Total de filas duplicadas: 0\n",
      "\n",
      "üöÆ 0 filas con valores nulos eliminadas.\n",
      "\n",
      "üöÆ 0 filas duplicadas eliminadas.\n",
      "\n",
      "‚úÖ Datos limpiados correctamente.\n",
      "\n",
      "üìå Columnas renombradas correctamente.\n",
      "\n",
      "üîé Filas finales despu√©s de limpieza: 1048575\n",
      "\n",
      "‚úÖ Archivo guardado en Parquet: C:\\\\Proyecto Final Henry\\\\DataBase\\\\processed\\\\Ventas.parquet\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def cargar_csv(ruta_csv6):\n",
    "    \"\"\"Carga un archivo CSV en un DataFrame de Pandas.\"\"\"\n",
    "    df6 = pd.read_csv(ruta_csv6)\n",
    "    print(f\"‚úÖ Archivo cargado: {ruta_csv6}\")\n",
    "    return df6\n",
    "\n",
    "def revisar_datos(df6):\n",
    "    \"\"\"Revisa el total de filas y columnas, valores faltantes y duplicados.\"\"\"\n",
    "    print(\"\\nüîç Revisi√≥n inicial de los datos:\")\n",
    "    print(f\"Forma del DataFrame (filas, columnas): {df6.shape}\")\n",
    "    print(\"\\nValores faltantes por columna:\\n\", df6.isnull().sum())\n",
    "\n",
    "    duplicados = df6.duplicated().sum()\n",
    "    print(f\"\\nTotal de filas duplicadas: {duplicados}\")\n",
    "\n",
    "def limpiar_datos(df6):\n",
    "    \"\"\"Elimina valores nulos y filas duplicadas para limpiar la base de datos.\"\"\"\n",
    "    # Eliminar filas con valores nulos\n",
    "    filas_nulas_totales = df6.isnull().any(axis=1).sum()\n",
    "    df6 = df6.dropna()\n",
    "    print(f\"\\nüöÆ {filas_nulas_totales} filas con valores nulos eliminadas.\")\n",
    "\n",
    "    # Eliminar duplicados\n",
    "    filas_duplicadas = df6.duplicated().sum()\n",
    "    df6 = df6.drop_duplicates()\n",
    "    print(f\"\\nüöÆ {filas_duplicadas} filas duplicadas eliminadas.\")\n",
    "\n",
    "    print(\"\\n‚úÖ Datos limpiados correctamente.\")\n",
    "    return df6  \n",
    "\n",
    "def renombrar_columnas(df6):\n",
    "    \"\"\"Renombra las columnas del DataFrame seg√∫n un formato est√°ndar.\"\"\"\n",
    "    df6.columns = [\n",
    "        \"InventarioInicialId\", \"Tienda\", \"ProductoId\", \"NombreProducto\", \"Tamano\",\n",
    "        \"CantidadVendida\", \"TotalVenta\", \"PrecioUnitario\", \"FechaVenta\", \"Volumen\",\n",
    "        \"Clasificacion\", \"Impuestos\", \"ProveedorId\", \"NombreProveedor\", \"Costo\"\n",
    "    ]\n",
    "    print(\"\\nüìå Columnas renombradas correctamente.\")\n",
    "    return df6  \n",
    "\n",
    "def guardar_parquet(df6, output_path):\n",
    "    \"\"\"Guarda el DataFrame en formato Parquet con compresi√≥n Snappy.\"\"\"\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    df6.to_parquet(output_path, engine=\"pyarrow\", compression=\"snappy\")\n",
    "    print(f\"\\n‚úÖ Archivo guardado en Parquet: {output_path}\")\n",
    "\n",
    "\n",
    "# ------------------ üîπ EJECUCI√ìN DEL PROCESO ETL üîπ ------------------\n",
    "ruta_csv6 = r\"C:\\\\Proyecto Final Henry\\\\DataBase\\\\SalesFINAL123120161.csv\"\n",
    "ruta_parquet6 = r\"C:\\\\Proyecto Final Henry\\\\DataBase\\\\processed\\\\Ventas.parquet\"\n",
    "\n",
    "# 1Ô∏è‚É£ Cargar datos\n",
    "df6 = cargar_csv(ruta_csv6)\n",
    "\n",
    "# 2Ô∏è‚É£ Revisar datos\n",
    "revisar_datos(df6)\n",
    "\n",
    "# 3Ô∏è‚É£ Limpiar datos\n",
    "df6 = limpiar_datos(df6)\n",
    "\n",
    "# 4Ô∏è‚É£ Renombrar columnas\n",
    "df6 = renombrar_columnas(df6)\n",
    "\n",
    "# 5Ô∏è‚É£ Verificar el n√∫mero de filas despu√©s de la limpieza\n",
    "print(f\"\\nüîé Filas finales despu√©s de limpieza: {df6.shape[0]}\")\n",
    "\n",
    "# 6Ô∏è‚É£ Guardar en Parquet\n",
    "guardar_parquet(df6, ruta_parquet6)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   InventarioInicialId  Tienda  ProductoId              NombreProducto  \\\n",
      "0  1_HARDERSFIELD_1004       1        1004  Jim Beam w/2 Rocks Glasses   \n",
      "1  1_HARDERSFIELD_1004       1        1004  Jim Beam w/2 Rocks Glasses   \n",
      "2  1_HARDERSFIELD_1004       1        1004  Jim Beam w/2 Rocks Glasses   \n",
      "3  1_HARDERSFIELD_1004       1        1004  Jim Beam w/2 Rocks Glasses   \n",
      "4  1_HARDERSFIELD_1005       1        1005     Maker's Mark Combo Pack   \n",
      "\n",
      "       Tamano  CantidadVendida  TotalVenta  PrecioUnitario FechaVenta  \\\n",
      "0       750mL                1       16.49           16.49   1/1/2016   \n",
      "1       750mL                2       32.98           16.49   1/2/2016   \n",
      "2       750mL                1       16.49           16.49   1/3/2016   \n",
      "3       750mL                1       14.49           14.49   1/8/2016   \n",
      "4  375mL 2 Pk                2       69.98           34.99   1/9/2016   \n",
      "\n",
      "   Volumen  Clasificacion  Impuestos  ProveedorId  \\\n",
      "0      750              1       0.79        12546   \n",
      "1      750              1       1.57        12546   \n",
      "2      750              1       0.79        12546   \n",
      "3      750              1       0.79        12546   \n",
      "4      375              1       0.79        12546   \n",
      "\n",
      "               NombreProveedor  Costo  \n",
      "0  JIM BEAM BRANDS COMPANY      10.65  \n",
      "1  JIM BEAM BRANDS COMPANY      10.65  \n",
      "2  JIM BEAM BRANDS COMPANY      10.65  \n",
      "3  JIM BEAM BRANDS COMPANY      10.65  \n",
      "4  JIM BEAM BRANDS COMPANY      27.34  \n",
      "Index(['InventarioInicialId', 'Tienda', 'ProductoId', 'NombreProducto',\n",
      "       'Tamano', 'CantidadVendida', 'TotalVenta', 'PrecioUnitario',\n",
      "       'FechaVenta', 'Volumen', 'Clasificacion', 'Impuestos', 'ProveedorId',\n",
      "       'NombreProveedor', 'Costo'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Cargar un archivo Parquet\n",
    "df = pd.read_parquet(\"C:\\\\Proyecto Final Henry\\\\DataBase\\\\processed\\\\Ventas.parquet\")\n",
    "\n",
    "# Mostrar las primeras filas\n",
    "print(df.head())\n",
    "\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subir archivo al bucket a la carpeta Datos Limpios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ Archivo C:\\\\Proyecto Final Henry\\\\DataBase\\\\processed\\\\Ventas.parquet subido a gs://licoreradatos/Datos Limpios/Ventas.parquet\n"
     ]
    }
   ],
   "source": [
    "def subir_a_gcs(bucket_name, source_file_path6, folder_name, destination_file_name6):\n",
    "    \"\"\"Sube un archivo a una carpeta espec√≠fica dentro de un bucket en Google Cloud Storage.\"\"\"\n",
    "    \n",
    "    # Inicializar el cliente de GCS\n",
    "    storage_client = storage.Client()\n",
    "    \n",
    "    # Obtener el bucket\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    \n",
    "    # Definir la ruta completa en el bucket (carpeta + nombre de archivo)\n",
    "    destination_blob_name = f\"{folder_name}/{destination_file_name6}\".strip(\"/\")\n",
    "    \n",
    "    # Crear un blob (objeto en el bucket)\n",
    "    blob = bucket.blob(destination_blob_name)\n",
    "    \n",
    "    # Subir el archivo\n",
    "    blob.upload_from_filename(source_file_path6)\n",
    "\n",
    "    print(f\"\\nüöÄ Archivo {source_file_path6} subido a gs://{bucket_name}/{destination_blob_name}\")\n",
    "\n",
    "# Datos espec√≠ficos de tu bucket y archivo\n",
    "bucket_name = \"licoreradatos\"  # Tu bucket en GCS\n",
    "folder_name = \"Datos Limpios\"  # Carpeta dentro del bucket\n",
    "source_file_path6 = r\"C:\\\\Proyecto Final Henry\\\\DataBase\\\\processed\\\\Ventas.parquet\"  # Archivo local\n",
    "destination_file_name6 = \"Ventas.parquet\"  # Nombre final en el bucket\n",
    "\n",
    "# Subir archivo a GCS\n",
    "subir_a_gcs(bucket_name, source_file_path6, folder_name, destination_file_name6)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
